# üó£Ô∏è BERT Word Prediction - AAC App

An advanced **Augmentative and Alternative Communication (AAC)** app for iPad that uses **BERT (Bidirectional Encoder Representations from Transformers)** to provide intelligent, context-aware word predictions for building sentences.

![iOS](https://img.shields.io/badge/iOS-15.0+-blue.svg)
![Swift](https://img.shields.io/badge/Swift-5.9-orange.svg)
![BERT](https://img.shields.io/badge/Model-BERT--base-green.svg)
![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)

---

## ‚ú® Features

### ü§ñ AI-Powered Predictions
- **BERT-base model** (110M parameters) for contextual understanding
- **15 word suggestions** ranked by probability
- **Bidirectional attention** - sees full sentence context
- **On-device inference** - no internet required, privacy-first

### üì± User Interface
- **Large, easy-to-tap buttons** for accessibility
- **Probability display** (testing mode) shows model confidence
- **Letter filtering** to narrow down word choices
- **Sentence display** with delete and clear functions
- **Responsive grid layout** optimized for iPad

### üéØ Technical Highlights
- **Core ML integration** for fast on-device inference (~50-100ms)
- **WordPiece tokenization** compatible with BERT
- **Masked Language Modeling (MLM)** approach
- **Softmax probability calculation** with numerical stability
- **Smart word filtering** removes punctuation and invalid tokens

---

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    SwiftUI Interface                         ‚îÇ
‚îÇ                   (ContentView.swift)                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ               Word Prediction Service                        ‚îÇ
‚îÇ          (WordPredictionService.swift)                       ‚îÇ
‚îÇ  ‚Ä¢ Tokenization                                              ‚îÇ
‚îÇ  ‚Ä¢ Model inference                                           ‚îÇ
‚îÇ  ‚Ä¢ Probability calculation                                   ‚îÇ
‚îÇ  ‚Ä¢ Word filtering                                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                  BERT Tokenizer                              ‚îÇ
‚îÇ              (GPT2Tokenizer.swift ‚Üí BERTTokenizer)          ‚îÇ
‚îÇ  ‚Ä¢ WordPiece tokenization                                    ‚îÇ
‚îÇ  ‚Ä¢ Special token handling ([CLS], [MASK], [SEP])           ‚îÇ
‚îÇ  ‚Ä¢ Attention mask generation                                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                Core ML Model                                 ‚îÇ
‚îÇ            (WordPredictor.mlpackage)                         ‚îÇ
‚îÇ  ‚Ä¢ BERT-base-uncased                                         ‚îÇ
‚îÇ  ‚Ä¢ Input: [batch_size=1, seq_len=128]                      ‚îÇ
‚îÇ  ‚Ä¢ Output: [batch_size=1, seq_len=128, vocab_size=30522]  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìã Requirements

- **macOS** with Xcode 15.0+
- **iPad** running iOS 15.0 or later
- **Apple ID** (free account works)
- **Python 3.9+** (for model conversion only)

---

## üöÄ Quick Start

### 1. Clone the Repository

```bash
git clone https://github.com/stevendisano/bert-word-prediction.git
cd bert-word-prediction
```

### 2. Install Dependencies (for model conversion)

```bash
pip install transformers coremltools torch
```

### 3. Convert BERT Model to Core ML ‚ö†Ô∏è REQUIRED

**The model weights are not included in the repo due to GitHub's 100MB file size limit.**

You must generate the Core ML model before running the app:

```bash
cd Predict
python3 convert_model.py
```

This will:
- Download `bert-base-uncased` from Hugging Face (~440MB)
- Convert it to Core ML format (~420MB)
- Save to `Predict/WordPredictor.mlpackage/`

**First time setup takes ~2-5 minutes** depending on your internet speed.

### 4. Deploy to iPad

Follow the detailed instructions in **[DEPLOYMENT.md](DEPLOYMENT.md)**

**Quick steps:**
1. Open `Predict/Predict.xcodeproj` in Xcode
2. Connect your iPad via USB
3. Select your iPad as the target device
4. Configure code signing with your Apple ID
5. Click Run (‚ñ∂Ô∏è)

---

## üìÇ Project Structure

```
Ipad/
‚îú‚îÄ‚îÄ Predict/
‚îÇ   ‚îú‚îÄ‚îÄ Predict.xcodeproj/          # Xcode project file
‚îÇ   ‚îú‚îÄ‚îÄ Predict/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ PredictApp.swift        # App entry point
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ContentView.swift       # Main UI (SwiftUI)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ WordPredictionService.swift  # Prediction logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ GPT2Tokenizer.swift     # BERT tokenizer (renamed)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ WordPredictor.mlpackage/     # Core ML BERT model
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vocab.json              # BERT vocabulary (30,522 tokens)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ merges.txt              # Placeholder for BERT
‚îÇ   ‚îú‚îÄ‚îÄ convert_model.py            # Python script to convert BERT
‚îÇ   ‚îî‚îÄ‚îÄ MODEL_SETUP.md              # Model conversion guide
‚îú‚îÄ‚îÄ DEPLOYMENT.md                    # iPad deployment instructions
‚îú‚îÄ‚îÄ README.md                        # This file
‚îî‚îÄ‚îÄ .gitignore                       # Git ignore rules
```

---

## üî¨ How It Works

### 1. **Tokenization**
User input "I asked" is converted to token IDs:
```
[CLS] "i" "asked" [MASK] "and" [SEP] [PAD] [PAD] ...
[101]  [1045] [2356] [103] [1998] [102]  [0]   [0]  ...
```

### 2. **BERT Inference**
- Model processes the entire sequence with bidirectional attention
- Generates logits for each position: `[1, 128, 30522]`
- Extracts logits at the `[MASK]` position (position 3)

### 3. **Probability Calculation**
- Apply softmax to convert logits ‚Üí probabilities
- Sort by probability (highest first)
- Filter out punctuation and invalid tokens

### 4. **Word Display**
- Top 15 words shown with probabilities
- Example for "I asked":
  - **Him** (5.6%)
  - **Her** (2.6%)
  - **Quietly** (0.9%)

### 5. **Context Word Trick**
Adding "and" after `[MASK]` prevents punctuation predictions:
- Without: `[CLS] "i" "asked" [MASK] [SEP]` ‚Üí predicts `.` (81%)
- With: `[CLS] "i" "asked" [MASK] "and" [SEP]` ‚Üí predicts `him` (5.6%)

---

## üìä Model Details

| Property | Value |
|----------|-------|
| **Model** | BERT-base-uncased |
| **Parameters** | 110 million |
| **Vocabulary Size** | 30,522 tokens |
| **Max Sequence Length** | 128 tokens |
| **Tokenization** | WordPiece |
| **Framework** | Core ML (converted from PyTorch) |
| **Inference Time** | ~50-100ms on iPad |
| **Model Size** | ~420MB |

---

## üé® UI Screenshots

### Main Interface
- Sentence display area at top
- 15 word prediction buttons (5x3 grid)
- Letter filter buttons at bottom
- Delete and Clear All controls

### Features
- **Probability display**: Shows model confidence (testing mode)
- **Letter filtering**: Tap a letter to filter words
- **Contextual predictions**: Suggestions change based on sentence

---

## üß™ Testing

### Test BERT Predictions (Python)

```bash
python3 test_bert_prediction.py
```

This shows raw BERT output with probabilities for test sentences.

### Monitor App Logs

```bash
xcrun simctl spawn booted log stream --predicate 'processImagePath contains "Predict"' --level debug
```

---

## üîß Configuration

### Customize Starting Words

Edit `WordPredictionService.swift` (lines 87-89):

```swift
let words = [
    "I", "The", "It", "You", "We", "What", "This", "Can", 
    "How", "There", "When", "My", "If", "He", "She"
]
```

### Hide Probability Display

Comment out in `ContentView.swift` (lines 86-90):

```swift
// if prediction.probability > 0 {
//     Text(String(format: "%.1f%%", prediction.probability * 100))
//         .font(.system(size: 12, weight: .medium))
//         .foregroundColor(.white.opacity(0.8))
// }
```

### Adjust Number of Predictions

Change in `WordPredictionService.swift` (line 177):

```swift
let result = Array(wordPredictions.prefix(15))  // Change 15 to desired number
```

---

## üêõ Troubleshooting

### "Core ML model not available"
- Ensure `WordPredictor.mlpackage` exists in `Predict/Predict/`
- Run `python3 convert_model.py` to regenerate the model

### "Failed to code sign"
- Add your Apple ID in Xcode ‚Üí Settings ‚Üí Accounts
- Enable "Automatically manage signing" in project settings

### "iPad not detected"
- Connect iPad via USB
- Unlock iPad and trust the computer
- Check: Window ‚Üí Devices and Simulators in Xcode

### Slow predictions
- First prediction is slower (~200ms) due to model loading
- Subsequent predictions are fast (~50-100ms)
- Performance depends on iPad model (newer = faster)

---

## üöß Known Limitations

1. **Vocabulary Coverage**: BERT-base has 30,522 tokens, may not include rare/slang words
2. **Sentence Length**: Limited to 128 tokens (truncated if longer)
3. **Single Word Prediction**: Currently predicts one word at a time
4. **Model Size**: 420MB - requires significant storage
5. **First Launch**: Takes a few seconds to load model

---

## üîÆ Future Enhancements

- [ ] Multi-word phrase predictions
- [ ] User vocabulary customization
- [ ] Prediction history and learning
- [ ] Voice output (text-to-speech)
- [ ] Smaller model option (DistilBERT)
- [ ] Fine-tuning on AAC-specific corpus
- [ ] Dark mode support
- [ ] Accessibility improvements (VoiceOver)

---

## üìö Technical Details

### Why BERT over GPT-2?

**BERT (Bidirectional):**
- ‚úÖ Sees context before AND after the mask
- ‚úÖ Better for filling in blanks (MLM task)
- ‚úÖ More natural for single-word prediction

**GPT-2 (Causal/Left-to-right):**
- ‚ùå Only sees context before the current position
- ‚ùå Designed for continuous text generation
- ‚ùå Tends to predict low-probability tokens

### Softmax Numerical Stability

```swift
let maxLogit = logits.max() ?? 0.0
let expValues = logits.map { exp($0 - maxLogit) }
let sumExp = expValues.reduce(0, +)
let probability = expValue / sumExp
```

Subtracting `maxLogit` prevents overflow in `exp()` calculation.

---

## üìñ Resources

- [BERT Paper](https://arxiv.org/abs/1810.04805) - Original BERT research
- [Hugging Face BERT](https://huggingface.co/bert-base-uncased) - Pre-trained model
- [Core ML Documentation](https://developer.apple.com/documentation/coreml) - Apple's ML framework
- [AAC Research](https://www.asha.org/practice-portal/professional-issues/augmentative-and-alternative-communication/) - Communication aids

---

## ü§ù Contributing

Contributions are welcome! Areas of interest:

- Model optimization (quantization, pruning)
- UI/UX improvements for accessibility
- Additional language support
- Performance benchmarking
- Bug fixes and testing

---

## üìÑ License

This project is licensed under the **Apache License 2.0**.

The BERT model (`bert-base-uncased`) is licensed under Apache 2.0 by Google and Hugging Face.

---

## üë§ Author

**Steven DiSano**
- GitHub: [@stevendisano](https://github.com/stevendisano)

---

## üôè Acknowledgments

- **Google Research** - BERT model
- **Hugging Face** - Pre-trained models and transformers library
- **Apple** - Core ML framework
- **AAC Community** - Inspiration and feedback

---

## üìÆ Support

For questions or issues:
1. Check [DEPLOYMENT.md](DEPLOYMENT.md) for setup help
2. Review [Troubleshooting](#-troubleshooting) section
3. Open an issue on GitHub

---

**Built with ‚ù§Ô∏è for augmentative and alternative communication**
